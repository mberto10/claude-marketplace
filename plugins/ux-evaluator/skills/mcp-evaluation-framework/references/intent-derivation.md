# Intent Derivation

How to derive natural conversation flows and user intents from product concepts for MCP app evaluation.

---

## Overview

For MCP app evaluation, the user journey is a conversation, not a click path. Before walking the UI, derive:

1. **Target persona** - Who is the user, derived from product concept
2. **Value context** - What the product promises and what alternatives exist
3. **Natural first ask** - How would they express their intent
4. **Conversation arc** - Expected turns from intent to resolution
5. **Success criteria** - What outcome satisfies the intent and delivers value

---

## Step 1: Extract Persona from Product Concept

### Required Information

From the product concept, extract:

```
PERSONA EXTRACTION
━━━━━━━━━━━━━━━━━

WHO
- Job title/role: [From target_user field]
- Domain expertise: [Novice/Intermediate/Expert]
- Technical comfort: [Low/Medium/High]

CONTEXT
- When they use the product: [Trigger situation]
- What they're trying to accomplish: [Job-to-be-done]
- What constraints they have: [Time, budget, requirements]

BEHAVIOR
- How they express themselves: [Terse/Detailed/Conversational]
- Patience level: [Low/Medium/High]
- Trust starting point: [Skeptical/Neutral/Trusting]
```

### Example Extraction

**Product Concept:**
```yaml
product_name: "FlightFinder Pro"
value_proposition: "Find the best flights for business travelers in seconds"
target_user: "Busy executives who need to book last-minute business travel"
core_loop: "Enter destination → See options → Book"
```

**Derived Persona:**
```
WHO
- Job title/role: Executive, busy professional
- Domain expertise: Intermediate (knows airports, not all airline nuances)
- Technical comfort: High (uses apps daily)

CONTEXT
- When they use: Last-minute travel needs, often urgent
- What they're accomplishing: Book a flight quickly with constraints
- Constraints: Time-sensitive, may have budget limits, prefer certain airlines/seats

BEHAVIOR
- Expression: Terse, goal-oriented ("flights to NYC tomorrow")
- Patience: Low (busy, needs speed)
- Trust: Neutral (will trust if it works, abandon if slow)
```

---

## Step 1.5: Extract Value Context

After identifying the persona, understand what value they expect and what they're comparing against.

### Value Proposition

From the product concept, extract:

```
VALUE CONTEXT
━━━━━━━━━━━━━

WHAT THE PRODUCT PROMISES:
[The core value proposition from the product concept]

SUCCESS CRITERIA:
[What does success look like for this persona?]
[How would they know they got value?]

ALTERNATIVES:
- Manual approach: [What would they do without this app?]
- Other tools: [What else could solve this problem?]

WHY THIS APP SHOULD WIN:
[What advantage should this MCP app provide?]
[Speed? Accuracy? Ease? Capability?]
```

### Example Value Context

**Product Concept:**
```yaml
product_name: "FlightFinder Pro"
value_proposition: "Find the best flights for business travelers in seconds"
```

**Derived Value Context:**
```
WHAT THE PRODUCT PROMISES:
Find the BEST flights (not just any flights) in SECONDS (not minutes)

SUCCESS CRITERIA:
- User gets a clear recommendation, not a haystack
- Total time from intent to booking < 2 minutes
- User confident they got a good deal

ALTERNATIVES:
- Manual: Go to airline websites, compare prices, takes 15-30 min
- Other tools: Google Flights, Kayak, travel agent

WHY THIS APP SHOULD WIN:
- Speed: Seconds vs. 15+ minutes
- Recommendation: "Best flight" vs. "here's 200 options"
- Integration: One conversation vs. multiple sites
```

This value context informs the evaluation: we're not just checking if the app works, but if it delivers on its promise better than alternatives.

---

## Step 2: Define Natural First Ask

### Principles

The first ask should be:
- **Natural** - How a real user would actually phrase it
- **Incomplete** - Real users don't provide all constraints upfront
- **Goal-oriented** - Focused on outcome, not interface

### Formula

```
"[Action verb] [object] [key constraint]"
```

### Examples by Domain

**Travel:**
- "Find me flights to Tokyo next month"
- "Book a hotel near the conference center"
- "What's the cheapest way to get to London?"

**E-commerce:**
- "I need running shoes for trail running"
- "Find a gift for my mom under $50"
- "Show me laptops good for video editing"

**Productivity:**
- "Schedule a meeting with the design team"
- "Summarize my emails from today"
- "Create a project plan for the website redesign"

**Finance:**
- "How are my investments doing?"
- "Pay my credit card bill"
- "Transfer $500 to savings"

### Anti-Patterns

**Too complete (unrealistic):**
```
"Find me a round-trip flight from SFO to NRT departing February 15
returning February 22, economy class, under $1000, prefer JAL or ANA,
aisle seat, with at least 2 hours layover"
```
Real users don't front-load this much. They iterate.

**Too vague (untestable):**
```
"Help me with travel"
```
Too ambiguous to evaluate specific tool→widget chain.

**Interface-aware (breaks immersion):**
```
"Use the flight search tool to query..."
```
Real users don't know about tools.

---

## Step 3: Map Conversation Arc

### Standard Arc Structure

```
TURN 1: INTENT EXPRESSION
User expresses initial goal
System: Acknowledges, may clarify

TURN 2: CONSTRAINT GATHERING
User provides/confirms constraints
System: Gathers needed parameters

TURN 3: RESULTS PRESENTATION
System shows options
User: Reviews, may refine

TURN 4: REFINEMENT (optional)
User adjusts parameters
System: Updates results

TURN 5: SELECTION/ACTION
User makes choice
System: Confirms action

TURN 6: RESOLUTION
System completes action
User: Receives confirmation
```

### Map Turns to Screens

Each turn maps to one or more UI screens:

```
CONVERSATION → SCREEN MAPPING
━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Turn 1: Intent → Landing / Search entry point
Turn 2: Constraints → Form / Filter panel
Turn 3: Results → Results list / Grid
Turn 4: Refinement → Modified results
Turn 5: Selection → Detail view / Confirmation
Turn 6: Resolution → Success / Receipt
```

### Example Arc

**Intent:** "Find me flights to Tokyo next month"

```
T1: "Find me flights to Tokyo next month"
    Screen: Search page with destination field
    System needs: destination, rough dates

T2: "Flying from San Francisco, flexible on exact dates, under $1000"
    Screen: Search form with all fields
    System needs: origin, date range, budget

T3: [Results displayed]
    Screen: Results list with 47 options
    User needs: Way to identify best option

T4: "Show me only direct flights"
    Screen: Filtered results
    User needs: Filter controls

T5: "Book the JAL flight on the 15th"
    Screen: Flight detail → Booking form
    User needs: Confirmation before commit

T6: [Booking confirmed]
    Screen: Confirmation page
    User needs: Receipt, next steps
```

---

## Step 4: Define Success Criteria

### Per-Turn Success

Define what success looks like at each turn:

```
TURN SUCCESS CRITERIA
━━━━━━━━━━━━━━━━━━━━

T1: Intent correctly captured
    - System understood destination (Tokyo, Japan - not Texas)
    - System understood timeframe (next month)
    - No unnecessary clarification

T2: Constraints gathered efficiently
    - Only asked what was needed
    - Used context where available (home city)
    - Confirmed assumptions

T3: Results serve the intent
    - Options match constraints
    - Best option identifiable
    - Not overwhelming

T4: Refinement works smoothly
    - Filter applied correctly
    - State preserved
    - Results update appropriately

T5: Selection is confirmed
    - Details clear before commit
    - Price/terms visible
    - Cancel option available

T6: Resolution is complete
    - Action succeeded
    - Confirmation provided
    - Next steps clear
```

### Overall Success

The evaluation succeeds if:
1. User could express intent naturally
2. Intent was correctly understood
3. Constraints were gathered efficiently (not over/under)
4. Results served the intent
5. User could refine without starting over
6. Action was confirmed before commit
7. Resolution was achieved and confirmed
8. **Value was delivered** - user got what they came for
9. **Better than alternatives** - this was faster/easier/better than doing it another way
10. **Would return** - user would use this app again

---

## Intent Categories

### Lookup Intent
"Show me X" / "What is X?" / "Find X"

**Characteristics:**
- Seeking information, not action
- Success = correct information displayed
- May need filters but not confirmation

**Expected arc:** 2-3 turns

### Search Intent
"Find me X where Y" / "I need X for Y"

**Characteristics:**
- Seeking options within constraints
- Success = relevant options presented
- Often needs refinement loop

**Expected arc:** 3-5 turns

### Comparison Intent
"Compare X and Y" / "Which is better, X or Y?"

**Characteristics:**
- Evaluating multiple options
- Success = side-by-side with decision-relevant data
- May need criteria specification

**Expected arc:** 3-4 turns

### Action Intent
"Do X" / "Book X" / "Send X" / "Create X"

**Characteristics:**
- Causing side effect
- Success = action completed with confirmation
- MUST have commit gate

**Expected arc:** 4-6 turns

### Modification Intent
"Change X to Y" / "Update X" / "Cancel X"

**Characteristics:**
- Altering existing state
- Success = change applied and confirmed
- May need confirmation for destructive changes

**Expected arc:** 3-4 turns

---

## Template: Intent Specification

Use this template before starting evaluation:

```markdown
# Intent Specification: [Name]

## Persona
- **Who:** [Role/title]
- **Expertise:** [Level]
- **Behavior:** [Terse/Detailed]
- **Patience:** [Level]

## Value Context
- **Product promise:** [What the app claims to do]
- **Success looks like:** [What outcome means value delivered]
- **Alternatives:** [What user would do without this app]
- **Why this should win:** [Expected advantage over alternatives]

## Intent
- **Category:** [Lookup/Search/Comparison/Action/Modification]
- **Natural first ask:** "[How user would say it]"
- **Underlying goal:** [What they really want to accomplish]
- **Key constraints:** [Budget, time, preferences]

## Expected Conversation Arc

| Turn | User Says/Does | System Should | Value Delivered |
|------|----------------|---------------|-----------------|
| T1 | [First ask] | [Response] | [Value at turn] |
| T2 | [Constraints] | [Gather/Confirm] | [Value at turn] |
| T3 | [Reviews] | [Show results] | [Value at turn] |
| T4 | [Refines] | [Update] | [Value at turn] |
| T5 | [Selects] | [Confirm] | [Value at turn] |
| T6 | [Completes] | [Resolve] | [Value at turn] |

## Success Criteria
- [ ] Intent correctly captured at T1
- [ ] Constraints gathered efficiently at T2
- [ ] Results serve intent at T3
- [ ] Refinement works at T4
- [ ] Confirmation before action at T5
- [ ] Clear resolution at T6
- [ ] **Value delivered** - user got what they came for
- [ ] **Better than alternatives** - faster/easier/better
- [ ] **Would return** - earned repeat use
```

---

## Common Pitfalls

### Pitfall: Over-Specified Intent
Starting with too much detail doesn't test the clarification flow.

**Fix:** Start vague, let the conversation add constraints.

### Pitfall: Interface-Aware Language
Using terms like "click," "button," "dropdown" breaks persona immersion.

**Fix:** Use natural goal language, not interface language.

### Pitfall: Single Happy Path
Only testing the perfect flow misses real user variance.

**Fix:** Also test: typos, wrong assumptions, change of mind, ambiguous input.

### Pitfall: Ignoring Failure Paths
Only testing success misses error handling quality.

**Fix:** Intentionally trigger errors and evaluate recovery.
