---
name: MCP Evaluation Framework
description: This skill should be used when the user asks to "evaluate MCP app", "check tool-to-widget flow", "find MCP failure patterns", "improve MCP tool schema", "evaluate conversational intent", "test MCP frontend", "assess MCP product fit", "check if MCP app delivers value", or needs guidance on intent-driven UI evaluation, product fit assessment, value delivery analysis, or improvement layer categorization for MCP-powered applications.
version: 0.2.0
---

# MCP Evaluation Framework

## Overview

This framework enables evaluation of MCP-powered applications for **product fit** - whether they deliver value that earns repeat use. For MCP apps, users arrive via LLM conversations. The evaluation walks the UI as the target persona, assessing whether the tool→widget chain delivers actual value.

**Core question:** "Is this an MCP product a user would want to use repeatedly because it brings them great value?"

This breaks down to:
1. **Value Proposition Delivery** - Does it do what it promises?
2. **Tool + Widget Value** - Does the combination actually help the user?
3. **Competitive Advantage** - Is it better than alternatives?
4. **Repeat Use Motivation** - Would the user return?

## Core Method

### 1. Derive Persona and Value Context

Extract from the product concept:

**Who is the target user?**
- Role, expertise, constraints
- How they express themselves

**What does the product promise?**
- Value proposition
- Success criteria
- Why they'd use this over alternatives

**What are their alternatives?**
- Manual approach (no tool)
- Existing tools
- What should make this MCP app better?

Become this persona. Know what success means to them and what they're comparing against.

### 2. Define Natural Conversation Flow

For the intent being evaluated:
- What would be their first message?
- What clarifications might they provide?
- What would a successful resolution look like?
- How would they know they got value?

This maps to UI screens being walked.

### 3. Walk UI with Turn-Based Evaluation

Navigate through the frontend using Playwright. At each screen, capture:

```
TURN [N] - SCREEN: [URL/State]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

INTENT AT THIS POINT:
[What the user is trying to accomplish]

HYPOTHESIZED TOOL CALL:
[What MCP tool would be called, with what parameters]

WHAT USER SEES:
[Widget/screen description]

VALUE DELIVERY ASSESSMENT:
- Tool result quality: [Complete? Accurate? Relevant?]
- Widget presentation: [Clear? Actionable? Appropriate?]
- Value delivered: [High/Partial/Low/None]
- Better than alternative? [Yes/No - why?]
- User feeling: [Confident/Neutral/Confused/Frustrated/Delighted]

FAILURE PATTERNS DETECTED:
[See references/failure-patterns.md]

IMPROVEMENT NEEDED:
Layer: [Tool Schema | Tool Output | Widget | Flow]
Specific: [What to change]
```

See `references/turn-evaluation-schema.md` for complete format.
See `references/value-assessment.md` for value delivery guidance.

### 4. Two Evaluation Modes

**Hypothetical Tracing Mode:**
- Infer tool calls from UI output and codebase inspection
- Reason about what parameters would be passed
- Evaluate based on visible widget output

**Actual Tool Calling Mode:**
- Call MCP tools directly via HTTP endpoint
- Verify actual outputs match expectations
- Compare actual output to what widget displays

### 5. Product Fit Assessment

After walking the complete flow, assess overall product fit:

```
PRODUCT FIT ASSESSMENT
━━━━━━━━━━━━━━━━━━━━━━

Value Proposition: [Delivered / Partial / Not Delivered]
vs. Alternatives: [Better / Same / Worse]
Would Return: [Definitely / Probably / Unlikely / No]
Would Recommend: [Definitely / Probably / Unlikely / No]

Overall Fit: [Strong / Moderate / Weak / No Fit]
```

See `references/value-assessment.md` for complete framework.

## Failure Patterns (Summary)

Seven MCP-specific failure patterns to detect:

| Pattern | Signal |
|---------|--------|
| **Over-clarifying** | Asking what could be inferred from context or tool call |
| **Under-clarifying** | Committing to action without sufficient constraints |
| **Tool ping-pong** | Multiple sequential calls that should batch |
| **Widget mismatch** | Wrong display type/timing for this intent |
| **Poor edit loop** | Cannot refine without starting over |
| **No commit gate** | Irreversible action without confirmation |
| **Error opacity** | Technical errors shown to user verbatim |

See `references/failure-patterns.md` for detailed descriptions and examples.

## Improvement Layers (Summary)

Categorize each improvement by which layer needs to change:

| Layer | What Changes | Example Fix |
|-------|--------------|-------------|
| **Tool Schema** | Parameter definitions, new params | Add `flexibility` param |
| **Tool Output** | Response structure, metadata | Include `recommended` flag |
| **Widget** | Display, controls, affordances | Add filter controls |
| **Flow** | Screen sequence, gates | Add confirmation step |

See `references/improvement-layers.md` for detailed categorization guidance.

## Report Structure

Follow the dogfooding report style with MCP-specific additions:

```markdown
# MCP Evaluation Report: [Flow/Intent]

**Product:** [Name]
**Product Fit:** [Strong / Moderate / Weak / No Fit]
**Intent Evaluated:** [The conversational intent]
**Persona:** [Derived from product concept]
**Mode:** [Hypothetical | Actual Tool Calling]
**Date:** [Timestamp]

---

## Executive Summary
[2-3 sentences: Does this MCP app deliver value that would make users return?]

---

## Persona & Value Context
[Who, what they want, their alternatives, success criteria]

---

## Turn-by-Turn Evaluation
[Each screen with value delivery assessment]

---

## Product Fit Assessment
- Value Proposition: [Delivered / Partial / Not Delivered]
- vs. Alternatives: [Better / Same / Worse]
- Would Return: [Rating + why]
- Would Recommend: [Rating + why]
- What works / What's missing for fit

---

## Failure Patterns Found
[Grouped by pattern type with evidence]

---

## Improvements by Layer
[Tool Schema | Tool Output | Widget | Flow]

---

## Priority Ranking
[Ranked by: blocks value > reduces value > friction]
```

## Integration with Technical Debugger

After evaluation, hand off to `technical-debugger` agent for code investigation:
- Provide the evaluation report as input
- Technical debugger traces each improvement to specific file:line locations
- Combined output gives both UX findings and code fixes

## Additional Resources

### Reference Files

For detailed guidance, consult:

- **`references/turn-evaluation-schema.md`** - Complete per-screen capture format with value delivery fields
- **`references/value-assessment.md`** - Product fit and value delivery assessment framework
- **`references/failure-patterns.md`** - Detailed pattern descriptions with examples
- **`references/improvement-layers.md`** - Layer categorization and fix guidance
- **`references/intent-derivation.md`** - How to derive conversation flows from product concepts
